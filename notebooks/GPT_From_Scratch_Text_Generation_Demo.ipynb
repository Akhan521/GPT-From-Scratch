{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#  üìö  GPT - From - Scratch: Testing Text Generation\n",
        "\n",
        "## üëãüèΩ Welcome!\n",
        "\n",
        "This notebook demonstrates the **inference and text generation** capabilities of my **custom GPT (Generative Pretrained Transformer)** model trained entirely **from scratch using PyTorch**.\n",
        "<br><br>\n",
        "\n",
        "## ‚ú® My Motivation\n",
        "\n",
        "Modern language models like ChatGPT and GPT-3 are powerful but often feel like black boxes that \"magically work\". I wanted to **demystify transformers** by understanding and building one from the ground up.\n",
        "\n",
        "> This project is the result of my deep dive:  \n",
        "1. I built my own GPT transformer architecture from scratch.  \n",
        "2. I trained it on a classic story that shaped my childhood, *Winnie the Pooh*.  \n",
        "3. I developed a pipeline to generate new text from scratch.\n",
        "\n",
        "<br>\n",
        "\n",
        "## üìñ What Did I Train On?\n",
        "\n",
        "I trained my model on the **first 6 chapters of A.A. Milne‚Äôs 'Winnie the Pooh'**, one of my favorite childhood stories. I chose this text not only for its **narrative richness and poetic tone**, but also because:\n",
        "\n",
        "> Winnie the Pooh shaped much of my childhood. I wanted my GPT model to learn from something meaningful, and I also wanted to have fun while doing it.\n",
        "\n",
        "My model was trained entirely on my **laptop using only a CPU**, so I carefully designed the architecture of my model to be lightweight yet capable.\n",
        "\n",
        "<br>\n",
        "\n",
        "## ü§ñ Architecture Overview\n",
        "\n",
        "| Component         | Value |\n",
        "|------------------|-------|\n",
        "| Tokenization     | Character-level |\n",
        "| Context Length   | 64 tokens |\n",
        "| Model Dimension  | 128 |\n",
        "| Attention Heads  | 4 |\n",
        "| Transformer Blocks | 4 |\n",
        "| Epochs           | 60 |\n",
        "| Device           | CPU (no GPU) |\n",
        "\n",
        "<br>I specifically chose this architecture to **balance quality and efficiency** under my tight hardware constraints.\n",
        "\n",
        "<br>\n",
        "\n",
        "## ‚öôÔ∏è My Technical Highlights\n",
        "\n",
        "- I designed and trained a transformer model **from scratch**.\n",
        "- I created a reusable `TextDataset` class with vocab serialization.\n",
        "- I wrote a `GPTTrainer` class for easy training, with code modularity in mind.\n",
        "- I incorporated a `temperature` parameter which controls the randomness in generation.\n",
        "- Inference works on **CPU and GPU**.\n",
        "> Note: Inference is the process of using a trained AI model to make predictions or generate new content based on some input data you provide.\n",
        "\n",
        "<br>\n",
        "\n",
        "## üëë Why This Matters?\n",
        "\n",
        "- It demonstrates my ability to **build foundational ML architecture**.\n",
        "- It highlights **strong software engineering** principles in practice.\n",
        "- It reflects my deep understanding of **transformer internals**.\n",
        "- It allows me to share what I've learned with you through a short demo!\n",
        "\n",
        "<br>\n",
        "\n",
        "## üöÄ Try It Yourself\n",
        "\n",
        "Without further-a-do, run each cell below, in order, to get started. Feel free to change the generation length or sampling temperature. Whether you're technical or not, this notebook lets you experience the creativity and control of my custom GPT model (trained on `Winnie the Pooh`).\n",
        "\n",
        "<br>\n",
        "\n",
        "**Author**: Aamir Khan  \n",
        "**GitHub**: [github.com/Akhan521/GPT-From-Scratch](https://github.com/Akhan521/GPT-From-Scratch)"
      ],
      "metadata": {
        "id": "6MOaT_WCxWOY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Dependencies.\n",
        "!pip install torch"
      ],
      "metadata": {
        "id": "yk-55x-kwbLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üìÅ Clone Project Files From GitHub.\n",
        "!git clone https://github.com/Akhan521/GPT-From-Scratch.git\n",
        "%cd GPT-From-Scratch"
      ],
      "metadata": {
        "id": "WX6DDTaox7dO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Necessary Modules.\n",
        "from model.gpt import GPT\n",
        "from training.config import TrainingConfig\n",
        "from training.dataset import TextDataset\n",
        "from generate import *\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# Set-up Our Device.\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Running on: {device}\")\n",
        "\n",
        "# Initialize our model configuration.\n",
        "config = TrainingConfig()\n"
      ],
      "metadata": {
        "id": "8OzMSRgOyHA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First, We'll Load Our Vocabulary Data.\n",
        "vocab_data = TextDataset.load_vocab(config.VOCAB_PATH)\n",
        "\n",
        "# Next, We'll Load My Pretrained Model.\n",
        "model = load_model(config.FINAL_MODEL_PATH, vocab_data, config)\n",
        "\n",
        "print(\"‚úÖ Model and vocabulary loaded.\")\n"
      ],
      "metadata": {
        "id": "iP7ZGbQq19Z_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Most Importantly, We'll Define Our Text-Generation Parameters Here!\n",
        "\n",
        "# The prompt we give our model, giving it a place to start generation from.\n",
        "prompt = 'Here is Edward Bear, coming'\n",
        "\n",
        "# The number of characters we want to generate.\n",
        "max_length = 1000\n",
        "\n",
        "# Our sampling temperature (controls how random text generation is).\n",
        "'''\n",
        "  -> T = 1.0: Normal sampling, balanced randomness.\n",
        "  -> T < 1.0: Less random, focused on the most likely characters to follow.\n",
        "  -> T > 1.0: More random, allows for exploration of less probable characters.\n",
        "'''\n",
        "temperature = 1.0\n",
        "\n",
        "'''\n",
        "NOTE:\n",
        "  As a heads up, if you modify the parameters above, be sure to re-run the cell in order to see your changes when generating new text.\n",
        "  Otherwise, you won't see your modified parameters reflected in your newly generated text.\n",
        "'''\n"
      ],
      "metadata": {
        "id": "PmkfypCm3EgL"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finally, We Test Our Model's Text Generation Capabilities.\n",
        "generate(model, vocab_data, prompt, max_length, temperature, config)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDprOUHpJLr0",
        "outputId": "6e717a0b-186f-4c51-fb4d-647a22540945"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating text...\n",
            "\n",
            "\tPrompt: Here is Edward Bear, coming\n",
            "\n",
            "\tMax Length: 1000 characters to generate.\n",
            "\n",
            "\tTemperature: 1.0 (controls randomness)\n",
            "\n",
            "============================================================\n",
            "\n",
            "Generated text:\n",
            "Here is Edward Bear, coming downstairs now, bump, bump, bump, bump, on the\n",
            "back of his head, behind Christopher Robin. It is, as far as he knows,\n",
            "the only way of coming downstairs, but sometimes he feels that there\n",
            "really is another way, if only he could stop bumping for a moment and\n",
            "think this humming it out.\n",
            "\n",
            "\"I was have decided to do something helpful about it, but didn't\n",
            "quite know what. So he decided to do comething helpful instead.\n",
            "\n",
            "\"Eeyore,\" he said solemnly, \"I, Winnie-the-Pooh, will find your tail for\n",
            "you.\"\n",
            "\n",
            "\"Thank you, Pooh,\" answered Eeyore. \"You're a real friend,\" said he.\n",
            "\"Not like Some,\" he said.\n",
            "\n",
            "So Winnie-the-Pooh went off to find Eeyore's tail.\n",
            "\n",
            "It was a fine spring morning in the forest as he started out. Little\n",
            "soft clouds played happily in a blue sky, skipping from time to time in\n",
            "front of the sun as if they had come to put it out, and dried it, while Owl licked the end of\n",
            "his pencil, and wondered how to spell \"birthday.\"\n",
            "\n",
            "\"Can you read, Pooh?\" he asked a little anxiously. \"There's a notice\n",
            "a\n",
            "\n",
            "============================================================\n",
            "\n",
            "Text generation completed successfully!\n",
            "\n"
          ]
        }
      ]
    }
  ]
}